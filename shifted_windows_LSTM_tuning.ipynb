{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact\n",
    "#import plotly.graph_objects as go\n",
    "import ruptures as rpt\n",
    "from itertools import combinations as comb\n",
    "#from statsmodels.stats import power\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "from tqdm import tqdm\n",
    "#import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from scipy.spatial.distance import cdist\n",
    "import colorsys\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpd_model = rpt.Binseg(model = 'l2',jump=500, min_size=4500)\n",
    "def change_point_detection(df, throw_away = 0.1):\n",
    "    \"\"\"\n",
    "    Changepoint Detection\n",
    "    Input: - df:  from read_data\n",
    "           - key: which is the filename\n",
    "           - throw_away: everything smaller than 10% of the total length of data is thrown away\n",
    "\n",
    "    Returns: - return_dict: A dictionary with all the data and the format key_cnt\n",
    "    \"\"\"\n",
    "    signal = df[['accelerometer_x','accelerometer_y','accelerometer_z']].values\n",
    "    length = len(df)\n",
    "    algo = cpd_model.fit(signal)\n",
    "    result = [0]\n",
    "    result += algo.predict(pen=1000)\n",
    "    if result[-1] != length:\n",
    "        result += [length]\n",
    "        \n",
    "    return_df = pd.DataFrame()\n",
    "    for i in range(len(result)-1):\n",
    "        if (result[i+1]-result[i]) > 4500:\n",
    "            if return_df.empty:\n",
    "                return_df = df.iloc[result[i]:result[i+1]]\n",
    "            else:\n",
    "                return_df = pd.concat([return_df, df.iloc[result[i]:result[i+1]]],ignore_index=True)\n",
    "       \n",
    "    \"\"\"\n",
    "    This is the seperated version, so we dont have windows that belong to different changepoint sections\n",
    "    return_dict = {}\n",
    "    cnt = 1\n",
    "    for i in range(len(result)-1):\n",
    "        if result[i+1]-result[i] > length*throw_away:\n",
    "            return_dict[key+'_'+str(cnt)] = df.iloc[result[i]:result[i+1]]\n",
    "            cnt+=1\n",
    "    \"\"\"\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'data'\n",
    "def read_data(filename):\n",
    "    accelerometer = pd.read_csv(os.path.join(folder_path, filename, 'Accelerometer.csv'),sep=';')\n",
    "    accelerometer['Time (s)'] = pd.to_datetime(accelerometer['Time (s)'], unit='s')\n",
    "    accelerometer =accelerometer.set_index('Time (s)')\n",
    "    accelerometer = accelerometer.resample('2.5ms').mean()\n",
    "    accelerometer.rename(columns={'Acceleration x (m/s^2)':'accelerometer_x','Acceleration y (m/s^2)':'accelerometer_y','Acceleration z (m/s^2)':'accelerometer_z'}, inplace=True)\n",
    "    #accelerometer = accelerometer[9000:-9000]\n",
    "    accelerometer.reset_index(inplace=True)\n",
    "\n",
    "    \n",
    "    gyroscope = pd.read_csv(os.path.join(folder_path, filename,'Gyroscope.csv'),sep=';')\n",
    "    gyroscope['Time (s)'] = pd.to_datetime(gyroscope['Time (s)'], unit='s')\n",
    "    gyroscope =gyroscope.set_index('Time (s)')\n",
    "    gyroscope = gyroscope.resample('2.5ms').mean()\n",
    "    gyroscope.rename(columns={'Gyroscope x (rad/s)':'gyroscope_x','Gyroscope y (rad/s)':'gyroscope_y','Gyroscope z (rad/s)':'gyroscope_z'}, inplace=True)\n",
    "    #gyroscope = gyroscope[9000:-9000]\n",
    "    gyroscope.reset_index(inplace=True)\n",
    "    merged = pd.merge(accelerometer,gyroscope, on= 'Time (s)', how='inner')\n",
    "    #return merged\n",
    "    return change_point_detection(merged)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex to delete the number at the end of the string:\n",
    "def delete_number(string):\n",
    "    return ''.join([i for i in string if not i.isdigit()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:23<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "data_dict = {}\n",
    "name_to_idx = {} # name -> idx\n",
    "i = 0\n",
    "\n",
    "for name in tqdm(sorted(os.listdir(folder_path))):\n",
    "    data_dict[name] = read_data(name)\n",
    "    data_dict[name].set_index('Time (s)', inplace=True)\n",
    "    data_dict[name].interpolate(inplace=True, method=\"time\")\n",
    "\n",
    "    # add label\n",
    "    if delete_number(name) not in name_to_idx.keys():\n",
    "        name_to_idx[delete_number(name)] = i\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(axis_list, axis, X_train, appendix=\"\"):\n",
    "    #mean\n",
    "    X_train[axis+'_mean'+appendix] = pd.Series(axis_list).apply(lambda x: x.mean())\n",
    "    #std dev\n",
    "    X_train[axis+'_std'+appendix] = pd.Series(axis_list).apply(lambda x: x.std())\n",
    "    #avg absolute difference\n",
    "    X_train[axis+'_aad'+appendix] = pd.Series(axis_list).apply(lambda x: np.mean(np.absolute(x - np.mean(x))))\n",
    "    #min\n",
    "    X_train[axis+'_min'+appendix] = pd.Series(axis_list).apply(lambda x: x.min())\n",
    "    #max\n",
    "    X_train[axis+'_max'+appendix] = pd.Series(axis_list).apply(lambda x: x.max())\n",
    "    #max-min diff\n",
    "    X_train[axis+'_maxmin_diff'+appendix] = X_train[axis+'_max'+appendix] - X_train[axis+'_min'+appendix]\n",
    "    #median\n",
    "    X_train[axis+'_median'+appendix] = pd.Series(axis_list).apply(lambda x: np.median(x))\n",
    "    #median absolut deviation\n",
    "    X_train[axis+'_mad'+appendix] = pd.Series(axis_list).apply(lambda x: np.median(np.absolute(x - np.median(x))))\n",
    "    #interquartile range\n",
    "    X_train[axis+'_IQR'+appendix] = pd.Series(axis_list).apply(lambda x: np.percentile(x, 75) - np.percentile(x, 25))\n",
    "\n",
    "    if appendix != \"_fft\":\n",
    "        #negative count\n",
    "        X_train[axis+'_neg_count'+appendix] = pd.Series(axis_list).apply(lambda x: np.sum(x < 0))\n",
    "        #positive count\n",
    "        X_train[axis+'_pos_count'+appendix] = pd.Series(axis_list).apply(lambda x: np.sum(x > 0))\n",
    "    \n",
    "    #values above mean\n",
    "    X_train[axis+'_above_mean'+appendix] = pd.Series(axis_list).apply(lambda x: np.sum(x > x.mean()))\n",
    "    #number of peaks\n",
    "    X_train[axis+'_peak_count'+appendix] = pd.Series(axis_list).apply(lambda x: len(find_peaks(x)[0]))\n",
    "    #skewness\n",
    "    X_train[axis+'_skewness'+appendix] = pd.Series(axis_list).apply(lambda x: stats.skew(x))\n",
    "    #kurtosis\n",
    "    X_train[axis+'_kurtosis'+appendix] = pd.Series(axis_list).apply(lambda x: stats.kurtosis(x))\n",
    "    # energy\n",
    "    X_train[axis+'_energy'+appendix] = pd.Series(axis_list).apply(lambda x: np.sum(x**2)/100)\n",
    "\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_keys = [] # \"nick2\", \"till2\", \"uta2\", \"paula2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chris': 0,\n",
       " 'felix': 1,\n",
       " 'katarina': 2,\n",
       " 'kirill': 3,\n",
       " 'leon': 4,\n",
       " 'leonie': 5,\n",
       " 'lucas': 6,\n",
       " 'luisa': 7,\n",
       " 'melna': 8,\n",
       " 'nele': 9,\n",
       " 'nick': 10,\n",
       " 'paula': 11,\n",
       " 'rebecca': 12,\n",
       " 'till': 13,\n",
       " 'uta': 14}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing lists\n",
    "x_list = []\n",
    "y_list = []\n",
    "z_list = []\n",
    "\n",
    "x_val_list = []\n",
    "y_val_list = []\n",
    "z_val_list = []\n",
    "val_labels = []\n",
    "\n",
    "train_labels = []\n",
    "x_test_list = []\n",
    "y_test_list = []\n",
    "z_test_list = []\n",
    "test_labels = []\n",
    "\n",
    "gx_list = []\n",
    "gy_list = []\n",
    "gz_list = []\n",
    "\n",
    "gx_val_list = []\n",
    "gy_val_list = []\n",
    "gz_val_list = []\n",
    "\n",
    "gx_test_list = []\n",
    "gy_test_list = []\n",
    "gz_test_list = []\n",
    "\n",
    "window_size = int(410 * 5) # we give the model 5 steps\n",
    "step_size = 410\n",
    "\n",
    "# Creating overlapping windows of size window_size\n",
    "for name, df_train in data_dict.items():\n",
    "    label = name_to_idx[delete_number(name)]\n",
    "    n_train_end = int(df_train.shape[0] * 0.7)  # 70% train\n",
    "    n_test_end = int(df_train.shape[0] * 0.85)  # next 15% test (and the last 15% for val)\n",
    "\n",
    "    # Training data\n",
    "    for i in range(0, n_train_end - window_size, step_size):\n",
    "        xs = df_train['accelerometer_x'].values[i: i + window_size]\n",
    "        ys = df_train['accelerometer_y'].values[i: i + window_size]\n",
    "        zs = df_train['accelerometer_z'].values[i: i + window_size]\n",
    "        x_list.append(xs)\n",
    "        y_list.append(ys)\n",
    "        z_list.append(zs)\n",
    "        train_labels.append(label)\n",
    "        \n",
    "        gxs = df_train['gyroscope_x'].values[i: i + window_size]\n",
    "        gys = df_train['gyroscope_y'].values[i: i + window_size]\n",
    "        gzs = df_train['gyroscope_z'].values[i: i + window_size]\n",
    "        gx_list.append(gxs)\n",
    "        gy_list.append(gys)\n",
    "        gz_list.append(gzs)\n",
    "    \n",
    "    # Testing data\n",
    "    for i in range(n_train_end, n_test_end - window_size, step_size):\n",
    "        xs = df_train['accelerometer_x'].values[i: i + window_size]\n",
    "        ys = df_train['accelerometer_y'].values[i: i + window_size]\n",
    "        zs = df_train['accelerometer_z'].values[i: i + window_size]\n",
    "        x_test_list.append(xs)\n",
    "        y_test_list.append(ys)\n",
    "        z_test_list.append(zs)\n",
    "        test_labels.append(label)\n",
    "\n",
    "        gxs = df_train['gyroscope_x'].values[i: i + window_size]\n",
    "        gys = df_train['gyroscope_y'].values[i: i + window_size]\n",
    "        gzs = df_train['gyroscope_z'].values[i: i + window_size]\n",
    "        gx_test_list.append(gxs)\n",
    "        gy_test_list.append(gys)\n",
    "        gz_test_list.append(gzs)\n",
    "\n",
    "    # Validation data\n",
    "    for i in range(n_test_end, df_train.shape[0] - window_size, step_size):\n",
    "        xs = df_train['accelerometer_x'].values[i: i + window_size]\n",
    "        ys = df_train['accelerometer_y'].values[i: i + window_size]\n",
    "        zs = df_train['accelerometer_z'].values[i: i + window_size]\n",
    "        x_val_list.append(xs)\n",
    "        y_val_list.append(ys)\n",
    "        z_val_list.append(zs)\n",
    "        val_labels.append(label)\n",
    "\n",
    "        gxs = df_train['gyroscope_x'].values[i: i + window_size]\n",
    "        gys = df_train['gyroscope_y'].values[i: i + window_size]\n",
    "        gzs = df_train['gyroscope_z'].values[i: i + window_size]\n",
    "        gx_val_list.append(gxs)\n",
    "        gy_val_list.append(gys)\n",
    "        gz_val_list.append(gzs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (9239, 2050, 6)\n",
      "X_val shape: (1903, 2050, 6)\n",
      "X_test shape: (1903, 2050, 6)\n",
      "y_train shape: (9239,)\n",
      "y_val shape: (1903,)\n",
      "y_test shape: (1903,)\n"
     ]
    }
   ],
   "source": [
    "def combine_data(acc_x, acc_y, acc_z, gyro_x, gyro_y, gyro_z):\n",
    "    combined_data = np.stack([acc_x, acc_y, acc_z, gyro_x, gyro_y, gyro_z], axis=2)\n",
    "    return combined_data\n",
    "\n",
    "def min_max_normalize(data):\n",
    "    min_val = np.nanmin(data, axis=1, keepdims=True)\n",
    "    max_val = np.nanmax(data, axis=1, keepdims=True)\n",
    "    normalized_data = (data - min_val) / (max_val - min_val + 1e-8)\n",
    "    return normalized_data\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "x_train_array = np.array(x_list)\n",
    "y_train_array = np.array(y_list)\n",
    "z_train_array = np.array(z_list)\n",
    "gx_train_array = np.array(gx_list)\n",
    "gy_train_array = np.array(gy_list)\n",
    "gz_train_array = np.array(gz_list)\n",
    "y_train = np.array(train_labels)\n",
    "\n",
    "x_val_array = np.array(x_val_list)\n",
    "y_val_array = np.array(y_val_list)\n",
    "z_val_array = np.array(z_val_list)\n",
    "gx_val_array = np.array(gx_val_list)\n",
    "gy_val_array = np.array(gy_val_list)\n",
    "gz_val_array = np.array(gz_val_list)\n",
    "y_val = np.array(val_labels)\n",
    "\n",
    "x_test_array = np.array(x_test_list)\n",
    "y_test_array = np.array(y_test_list)\n",
    "z_test_array = np.array(z_test_list)\n",
    "gx_test_array = np.array(gx_test_list)\n",
    "gy_test_array = np.array(gy_test_list)\n",
    "gz_test_array = np.array(gz_test_list)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "# Combine accelerometer and gyroscope data for each dataset\n",
    "X_train = combine_data(x_train_array, y_train_array, z_train_array, gx_train_array, gy_train_array, gz_train_array)\n",
    "X_val = combine_data(x_val_array, y_val_array, z_val_array, gx_val_array, gy_val_array, gz_val_array)\n",
    "X_test = combine_data(x_test_array, y_test_array, z_test_array, gx_test_array, gy_test_array, gz_test_array)\n",
    "\n",
    "# Normalize each sample in the datasets\n",
    "X_train = np.array([min_max_normalize(sample) for sample in X_train])\n",
    "X_val = np.array([min_max_normalize(sample) for sample in X_val])\n",
    "X_test = np.array([min_max_normalize(sample) for sample in X_test])\n",
    "\n",
    "# Shuffle the training data\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "# Print shapes to verify\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flattened = X_train.reshape(X_train.shape[0], -1)\n",
    "X_val_flattened = X_val.reshape(X_val.shape[0], -1)\n",
    "X_test_flattened = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window length vs avg F1 score data\n",
    "\n",
    "Window length, step size:\n",
    "Weighted avg of: Precision, Recall, F1 score\n",
    "\n",
    "- 410 * 5, 410\n",
    "XGBoost 0.99      0.99      0.99, LogisticRegression 0.97      0.97      0.97\n",
    "\n",
    "- 410 * 4, 410\n",
    "XGBoost 1.00      1.00      1.00, LogisticRegression 0.96      0.96      0.96\n",
    "\n",
    "- 410 * 3, 410\n",
    "XGBoost 1.00      1.00      1.00, LogisticRegression 0.96      0.96      0.96\n",
    "\n",
    "- 410 * 2, 410\n",
    "XGBoost 0.99      0.99      0.99, LogisticRegression 0.97      0.96      0.96\n",
    "\n",
    "- 410 * 1, 410\n",
    "XGBoost 0.99      0.99      0.99, LogisticRegression 0.91      0.89      0.90\n",
    "\n",
    "- 410 * 0.1, 410\n",
    "XGBoost 0.94      0.94      0.94, LogisticRegression 0.56      0.56      0.54\n",
    "\n",
    "- 410 * 0.01, 410\n",
    "XGBoost 0.80      0.80      0.79, LogisticRegression 0.30      0.30      0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import tensorflow as tf\\n\\n# Define the dimensions\\ninput_dim = X_train_flattened.shape[1]\\nlatent_dim = 2\\n\\n# Define the encoder\\nencoder = tf.keras.Sequential([\\n    tf.keras.layers.Input(shape=(input_dim,)),\\n    tf.keras.layers.Dense(1024, activation='relu'),\\n    tf.keras.layers.Dense(1024, activation='relu'),\\n    tf.keras.layers.Dense(512, activation='relu'),\\n    tf.keras.layers.Dense(512, activation='relu'),\\n    tf.keras.layers.Dense(256, activation='relu'),\\n    tf.keras.layers.Dense(256, activation='relu'),\\n    tf.keras.layers.Dense(128, activation='relu'),\\n    tf.keras.layers.Dense(128, activation='relu'),\\n    tf.keras.layers.Dense(latent_dim)  # No activation for direct latent representation\\n])\\n\\n# Define the decoder\\ndecoder = tf.keras.Sequential([\\n    tf.keras.layers.Input(shape=(latent_dim,)),\\n    tf.keras.layers.Dense(128, activation='relu'),\\n    tf.keras.layers.Dense(128, activation='relu'),\\n    tf.keras.layers.Dense(256, activation='relu'),\\n    tf.keras.layers.Dense(256, activation='relu'),\\n    tf.keras.layers.Dense(512, activation='relu'),\\n    tf.keras.layers.Dense(512, activation='relu'),\\n    tf.keras.layers.Dense(1024, activation='relu'),\\n    tf.keras.layers.Dense(1024, activation='relu'),\\n    tf.keras.layers.Dense(input_dim, activation='linear')  # Sigmoid to match input range\\n])\\n\\n# Combine encoder and decoder into autoencoder\\nautoencoder = tf.keras.Model(inputs=encoder.input, outputs=decoder(encoder.output))\\n\\n# Compile the model\\nautoencoder.compile(optimizer='adam', loss='mae')\\n\\n# Print summary\\nautoencoder.summary()\\n\\n# Train the autoencoder\\nhistory = autoencoder.fit(x=X_train_flattened, y=X_train_flattened, epochs=300, batch_size=512, validation_split=0.01)\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import tensorflow as tf\n",
    "\n",
    "# Define the dimensions\n",
    "input_dim = X_train_flattened.shape[1]\n",
    "latent_dim = 2\n",
    "\n",
    "# Define the encoder\n",
    "encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(input_dim,)),\n",
    "    tf.keras.layers.Dense(1024, activation='relu'),\n",
    "    tf.keras.layers.Dense(1024, activation='relu'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(latent_dim)  # No activation for direct latent representation\n",
    "])\n",
    "\n",
    "# Define the decoder\n",
    "decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(latent_dim,)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1024, activation='relu'),\n",
    "    tf.keras.layers.Dense(1024, activation='relu'),\n",
    "    tf.keras.layers.Dense(input_dim, activation='linear')  # Sigmoid to match input range\n",
    "])\n",
    "\n",
    "# Combine encoder and decoder into autoencoder\n",
    "autoencoder = tf.keras.Model(inputs=encoder.input, outputs=decoder(encoder.output))\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "# Print summary\n",
    "autoencoder.summary()\n",
    "\n",
    "# Train the autoencoder\n",
    "history = autoencoder.fit(x=X_train_flattened, y=X_train_flattened, epochs=300, batch_size=512, validation_split=0.01)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"plt.plot(history.history['loss'], label='loss')\\nplt.plot(history.history['val_loss'], label='val_loss')\\nplt.legend()\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"encoded_train = encoder.predict(X_train_flattened)\\n\\n# Plotting the encoded_train (latent representations) with y_train as color\\nplt.figure(figsize=(10, 8))\\nsns.scatterplot(x=encoded_train[:, 0], y=encoded_train[:, 1], hue=y_train, palette='tab20', s=50, alpha=0.8)\\nplt.title('Autoencoder Latent Representations')\\nplt.xlabel('Latent Dimension 1')\\nplt.ylabel('Latent Dimension 2')\\nplt.legend(title='Train Label', loc='best')\\nplt.tight_layout()\\nplt.show()\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"encoded_train = encoder.predict(X_train_flattened)\n",
    "\n",
    "# Plotting the encoded_train (latent representations) with y_train as color\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x=encoded_train[:, 0], y=encoded_train[:, 1], hue=y_train, palette='tab20', s=50, alpha=0.8)\n",
    "plt.title('Autoencoder Latent Representations')\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.legend(title='Train Label', loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-13 01:02:10.880125: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-13 01:02:10.899337: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-13 01:02:11.237342: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Define the dimensions\\ninput_dim = X_train_flattened.shape[1]\\nlatent_dim = 2\\n\\n# Sampling function for the latent space\\nclass Sampling(layers.Layer):\\n    def call(self, inputs):\\n        mean, log_var = inputs\\n        batch = tf.shape(mean)[0]\\n        dim = tf.shape(mean)[1]\\n        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\\n        return mean + tf.exp(0.5 * log_var) * epsilon\\n\\n# Define the encoder\\ninputs = layers.Input(shape=(input_dim,))\\nx = layers.Dense(1024, activation='relu')(inputs)\\nx = layers.Dense(1024, activation='relu')(x)\\nx = layers.Dense(512, activation='relu')(x)\\nx = layers.Dense(512, activation='relu')(x)\\nx = layers.Dense(256, activation='relu')(x)\\nx = layers.Dense(256, activation='relu')(x)\\nz_mean = layers.Dense(latent_dim, name='z_mean')(x)\\nz_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\\nz = Sampling()([z_mean, z_log_var])\\n\\nencoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\\nencoder.summary()\\n\\n# Define the decoder\\nlatent_inputs = layers.Input(shape=(latent_dim,))\\nx = layers.Dense(256, activation='relu')(latent_inputs)\\nx = layers.Dense(256, activation='relu')(x)\\nx = layers.Dense(512, activation='relu')(x)\\nx = layers.Dense(512, activation='relu')(x)\\nx = layers.Dense(1024, activation='relu')(x)\\nx = layers.Dense(1024, activation='relu')(x)\\noutputs = layers.Dense(input_dim, activation='sigmoid')(x)\\n\\ndecoder = Model(latent_inputs, outputs, name='decoder')\\ndecoder.summary()\\n\\n# Define the VAE\\noutputs = decoder(z)\\nvae = Model(inputs, outputs, name='vae')\\n\\n# Define the VAE loss\\nreconstruction_loss = tf.keras.losses.mae(inputs, outputs) * input_dim\\nkl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\\nvae_loss = tf.reduce_mean(reconstruction_loss + 1 * kl_loss)\\n\\nvae.add_loss(vae_loss)\\nvae.compile(optimizer='adam')\\n#vae.summary()\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Define the dimensions\n",
    "input_dim = X_train_flattened.shape[1]\n",
    "latent_dim = 2\n",
    "\n",
    "# Sampling function for the latent space\n",
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        batch = tf.shape(mean)[0]\n",
    "        dim = tf.shape(mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return mean + tf.exp(0.5 * log_var) * epsilon\n",
    "\n",
    "# Define the encoder\n",
    "inputs = layers.Input(shape=(input_dim,))\n",
    "x = layers.Dense(1024, activation='relu')(inputs)\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "\n",
    "# Define the decoder\n",
    "latent_inputs = layers.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(256, activation='relu')(latent_inputs)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "outputs = layers.Dense(input_dim, activation='sigmoid')(x)\n",
    "\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "\n",
    "# Define the VAE\n",
    "outputs = decoder(z)\n",
    "vae = Model(inputs, outputs, name='vae')\n",
    "\n",
    "# Define the VAE loss\n",
    "reconstruction_loss = tf.keras.losses.mae(inputs, outputs) * input_dim\n",
    "kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
    "vae_loss = tf.reduce_mean(reconstruction_loss + 1 * kl_loss)\n",
    "\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "#vae.summary()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Train the VAE\\nhistory = vae.fit(X_train_flattened, X_train_flattened, epochs=100, batch_size=512, validation_split=0.01)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Train the VAE\n",
    "history = vae.fit(X_train_flattened, X_train_flattened, epochs=100, batch_size=512, validation_split=0.01)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"plt.plot(history.history['loss'], label='loss')\\nplt.plot(history.history['val_loss'], label='val_loss')\\nplt.legend()\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Obtain the latent representations (encodings) for X_train\\nz_mean, _, _ = encoder.predict(X_train_flattened)\\n\\n# Plot the encodings with y_train as color\\nplt.figure(figsize=(10, 8))\\nsns.scatterplot(x=z_mean[:, 0], y=z_mean[:, 1], hue=y_train, palette='tab20', s=50, alpha=0.8)\\nplt.title('Variational Autoencoder Latent Representations')\\nplt.xlabel('Latent Dimension 1')\\nplt.ylabel('Latent Dimension 2')\\nplt.legend(title='Train Label', loc='best')\\nplt.tight_layout()\\nplt.show()\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Obtain the latent representations (encodings) for X_train\n",
    "z_mean, _, _ = encoder.predict(X_train_flattened)\n",
    "\n",
    "# Plot the encodings with y_train as color\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x=z_mean[:, 0], y=z_mean[:, 1], hue=y_train, palette='tab20', s=50, alpha=0.8)\n",
    "plt.title('Variational Autoencoder Latent Representations')\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.legend(title='Train Label', loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# X_top_features = X_test[top_features_]\\ntsne = TSNE(n_components=2, random_state=42, perplexity=30)\\nX_tsne = tsne.fit_transform(X_train_flattened)\\n\\nplt.figure(figsize=(8, 6))\\nsns.scatterplot(x=X_tsne[:,0], y=X_tsne[:,1], hue=y_train, palette='tab20', s=50, alpha=0.8)\\nplt.title('t-SNE Visualization of Top 12 Important Features')\\nplt.xlabel('t-SNE Component 1')\\nplt.ylabel('t-SNE Component 2')\\nplt.legend(title='Test Label', loc='best')\\nplt.tight_layout()\\nplt.show()\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# X_top_features = X_test[top_features_]\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "X_tsne = tsne.fit_transform(X_train_flattened)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X_tsne[:,0], y=X_tsne[:,1], hue=y_train, palette='tab20', s=50, alpha=0.8)\n",
    "plt.title('t-SNE Visualization of Top 12 Important Features')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend(title='Test Label', loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Step 2: Apply PCA to reduce dimensionality to 2D\\npca = KernelPCA(n_components=2, kernel='poly', degree=10, random_state=42)\\n# pca = PCA(n_components=2, random_state=0)\\nX_pca = pca.fit_transform(X_val_flattened)\\n\\n# Step 3: Plotting\\nplt.figure(figsize=(8, 6))\\nsns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y_val, palette='tab20', s=50, alpha=0.8)\\nplt.title('PCA Visualization of Top 12 Important Features')\\nplt.xlabel('PCA Component 1')\\nplt.ylabel('PCA Component 2')\\nplt.legend(title='Test Label', loc='best')\\n\\n# Set x-axis and y-axis limits based on the data\\nx_min, x_max = X_pca[:, 0].min(), X_pca[:, 0].max()\\ny_min, y_max = X_pca[:, 1].min(), X_pca[:, 1].max()\\nplt.xlim(x_min - 0.1*(x_max - x_min), x_max + 0.1*(x_max - x_min))\\nplt.ylim(y_min - 0.1*(y_max - y_min), y_max + 0.1*(y_max - y_min))\\n\\nplt.tight_layout()\\nplt.show()\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Step 2: Apply PCA to reduce dimensionality to 2D\n",
    "pca = KernelPCA(n_components=2, kernel='poly', degree=10, random_state=42)\n",
    "# pca = PCA(n_components=2, random_state=0)\n",
    "X_pca = pca.fit_transform(X_val_flattened)\n",
    "\n",
    "# Step 3: Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y_val, palette='tab20', s=50, alpha=0.8)\n",
    "plt.title('PCA Visualization of Top 12 Important Features')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend(title='Test Label', loc='best')\n",
    "\n",
    "# Set x-axis and y-axis limits based on the data\n",
    "x_min, x_max = X_pca[:, 0].min(), X_pca[:, 0].max()\n",
    "y_min, y_max = X_pca[:, 1].min(), X_pca[:, 1].max()\n",
    "plt.xlim(x_min - 0.1*(x_max - x_min), x_max + 0.1*(x_max - x_min))\n",
    "plt.ylim(y_min - 0.1*(y_max - y_min), y_max + 0.1*(y_max - y_min))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torchmetrics\n",
    "import os\n",
    "import ruptures as rpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(y_train)\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = torch.Tensor(self.sequences[idx])\n",
    "        label = torch.Tensor([self.labels[idx]]).long()\n",
    "        return dict(sequence=sequence, labels=label)\n",
    "\n",
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "val_dataset = TimeSeriesDataset(X_val, y_val)\n",
    "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=os.cpu_count(),persistent_workers=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=os.cpu_count(),persistent_workers=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=os.cpu_count(),persistent_workers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceModel(nn.Module):\n",
    "    def __init__(self, n_features, n_classes, n_hidden=512, n_layers=3, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=n_hidden,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.5\n",
    "        )\n",
    "        self.classifier = nn.Linear(n_hidden, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.lstm.flatten_parameters()\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        out = hidden[-1]\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N_EPOCHS = 1\\nmodel = LSTMPredictor(\\n    n_features=X_train.shape[2],\\n    n_classes=len(label_encoder.classes_),\\n    lr=0.01,\\n    max_epochs=3\\n)\\n\\ntrainer = pl.Trainer(\\n    max_epochs=model.hparams.max_epochs,\\n    callbacks=model.configure_callbacks(),\\n    accelerator=\"auto\"\\n)\\n\\ntrainer.fit(model, train_loader, val_loader)\\ntrainer.test(model, test_loader)'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTMPredictor(pl.LightningModule):\n",
    "    def __init__(self, n_features, n_classes, lr=0.001, max_epochs=10, n_hidden=512, n_layers=3, dropout=0.5, config=None):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = SequenceModel(n_features, n_classes, n_hidden, n_layers, dropout)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        output = self.model(x)\n",
    "        loss = self.criterion(output, labels) if labels is not None else torch.tensor(0)\n",
    "        return loss, output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        sequences = batch[\"sequence\"]\n",
    "        labels = batch[\"labels\"].squeeze()\n",
    "        loss, outputs = self(sequences, labels)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        step_f1 = torchmetrics.functional.f1_score(predictions, labels, task=\"multiclass\", num_classes=self.hparams.n_classes, average='weighted')\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        self.log(\"train_accuracy\", step_f1, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"f1_score\": step_f1}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        sequences = batch[\"sequence\"]\n",
    "        labels = batch[\"labels\"].squeeze()\n",
    "        loss, outputs = self(sequences, labels)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        step_f1 = torchmetrics.functional.f1_score(predictions, labels, task=\"multiclass\", num_classes=self.hparams.n_classes, average='weighted')\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        self.log(\"val_accuracy\", step_f1, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"f1_score\": step_f1}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        sequences = batch[\"sequence\"]\n",
    "        labels = batch[\"labels\"].squeeze()\n",
    "        loss, outputs = self(sequences, labels)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        step_f1 = torchmetrics.functional.f1_score(predictions, labels, task=\"multiclass\", num_classes=self.hparams.n_classes, average='weighted')\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        self.log(\"test_accuracy\", step_f1, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"f1_score\": step_f1}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "    def configure_callbacks(self):\n",
    "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "            dirpath=\"checkpoints\",\n",
    "            filename=\"best-checkpoint\",\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            save_top_k=1,\n",
    "            verbose=True\n",
    "        )\n",
    "        return [] # [checkpoint_callback]\n",
    "\n",
    "# Training\n",
    "\n",
    "\"\"\"N_EPOCHS = 1\n",
    "model = LSTMPredictor(\n",
    "    n_features=X_train.shape[2],\n",
    "    n_classes=len(label_encoder.classes_),\n",
    "    lr=0.01,\n",
    "    max_epochs=3\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=model.hparams.max_epochs,\n",
    "    callbacks=model.configure_callbacks(),\n",
    "    accelerator=\"auto\"\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'shifted_windows_LSTM_tuning.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 7ekwzu6g\n",
      "Sweep URL: https://wandb.ai/ds4w/ds4w/sweeps/7ekwzu6g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: akqzt1la with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epochs: 10\n",
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/till/Desktop/ds4w-user-identification/wandb/run-20240713_010853-akqzt1la</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ds4w/ds4w/runs/akqzt1la' target=\"_blank\">dutiful-sweep-1</a></strong> to <a href='https://wandb.ai/ds4w/ds4w' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ds4w/ds4w/sweeps/7ekwzu6g' target=\"_blank\">https://wandb.ai/ds4w/ds4w/sweeps/7ekwzu6g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ds4w/ds4w' target=\"_blank\">https://wandb.ai/ds4w/ds4w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ds4w/ds4w/sweeps/7ekwzu6g' target=\"_blank\">https://wandb.ai/ds4w/ds4w/sweeps/7ekwzu6g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ds4w/ds4w/runs/akqzt1la' target=\"_blank\">https://wandb.ai/ds4w/ds4w/runs/akqzt1la</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | SequenceModel    | 5.3 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "5.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.3 M     Total params\n",
      "21.101    Total estimated model params size (MB)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dropout' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                        | 0/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40584ae9cd945d3bd31818180889f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                               | 0/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                             | 0/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                             | 0/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                             | 0/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                             | 0/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                             | 0/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                             | 0/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                             | 0/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                             | 0/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                             | 0/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                             | 0/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dropout' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ae8f4491384675ae4f38a208318f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                                                                | 0/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_accuracy         0.14883427321910858\n",
      "        test_loss           2.6056838035583496\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▂▃▃▄▄▄▅▅▅▅▅▆▆▇▇▇▇▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▃█▄▅▅▄▁▄▇▇▂█▂▂</td></tr><tr><td>train_loss</td><td>▆█▁▆▇▇█▇▅▆▆▆▇▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇███</td></tr><tr><td>val_accuracy</td><td>▄▁████████</td></tr><tr><td>val_loss</td><td>▁▅████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.14883</td></tr><tr><td>test_loss</td><td>2.60568</td></tr><tr><td>train_accuracy</td><td>0.04625</td></tr><tr><td>train_loss</td><td>2.55728</td></tr><tr><td>trainer/global_step</td><td>730</td></tr><tr><td>val_accuracy</td><td>0.15045</td></tr><tr><td>val_loss</td><td>2.58999</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dutiful-sweep-1</strong> at: <a href='https://wandb.ai/ds4w/ds4w/runs/akqzt1la' target=\"_blank\">https://wandb.ai/ds4w/ds4w/runs/akqzt1la</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240713_010853-akqzt1la/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_evaluate(config=None):\n",
    "    if config is None:\n",
    "        config = {}  # Initialize config if not provided\n",
    "\n",
    "    pl.seed_everything(42)  # Ensure reproducibility\n",
    "\n",
    "    # Initialize wandb\n",
    "    wandb.init(config=config)\n",
    "    wandb.config.update(config)\n",
    "\n",
    "    # Create LightningModule instance\n",
    "    model = LSTMPredictor(\n",
    "        n_features=X_train.shape[2],\n",
    "        n_classes=len(label_encoder.classes_),\n",
    "        max_epochs=config.get('max_epochs', 10),\n",
    "        dropout=config.get('dropout', 0.5)\n",
    "    )\n",
    "\n",
    "    # Initialize Lightning Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=config.get('max_epochs', 10),\n",
    "        callbacks=model.configure_callbacks(),\n",
    "        logger=pl.loggers.WandbLogger(),\n",
    "        accelerator=\"auto\",\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    # Test the model\n",
    "    trainer.test(model, test_loader)\n",
    "\n",
    "    # Close wandb at the end of the run\n",
    "    wandb.finish()\n",
    "\n",
    "# Define sweep configuration\n",
    "sweep_config = {\n",
    "    'method': 'grid',\n",
    "    'metric': {\n",
    "        'name': 'val_accuracy',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'max_epochs': {\n",
    "            'value': 10\n",
    "        },\n",
    "        'dropout': {\n",
    "            'values': [0.3, 0.5, 0.7]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize the sweep\n",
    "sweep_id = wandb.sweep(sweep_config, entity='ds4w', project='ds4w')\n",
    "\n",
    "# Perform the sweep\n",
    "wandb.agent(sweep_id, function=train_evaluate, count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1ljvo44c with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epochs: 10\n",
      "Seed set to 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, function=train_evaluate, count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral",
   "language": "python",
   "name": "mistral"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
